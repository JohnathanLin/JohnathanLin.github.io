<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>简述爬虫对两种网站的不同爬取方式 | 风萧古道 - 勤学苦练，年复一年</title>
<meta name=keywords content><meta name=description content="爬虫的目的是采集网站的数据。而网站渲染数据有两种方式。我个人将其称为前端渲染和后端渲染。
前端渲染 前端渲染指的是网页并不直接展示数据，而是在读取完网页之后，再次向服务器请求数据。在得到数据之后再渲染到网页中。
后端渲染 后端渲染指的是服务器收到请求之后，将数据在后端写入网页，然后将带有数据的网页直接展示在浏览器中。
爬取方式 目前我并没有使用scrapy、webmagic等爬虫框架，仅使用python的requests模块，json模块和BeautifulSoup框架。
前端渲染的爬取方式
步骤：使用requests请求数据，再用json.loads()方法将返回的数据解析，最后操作得到的数据对象即可。
我们这里以豆瓣为例。
分析该网页。 爬取的url：
https://movie.douban.com/explore#!type=movie&amp;tag=%E7%83%AD%E9%97%A8&amp;sort=recommend&amp;page_limit=20&amp;page_start=0
将url放进浏览器，先点开F12，然后访问。这里我使用谷歌浏览器 可以看到，红框内的All、XHR、JS等。这是一个筛选框，用来筛选该网页请求的数据。
All代表所有，XHR代表异步请求，JS代表Js文件，Css……
对于前端渲染，必然有异步的过程，所以选择XHR。
通过观察，发现第三行是“选电影”列表的数据。 这时，我们点击Headers，查看其请求的详细信息。 可以看到，这是一个Get请求，在下面的Qurey String Parameters可以看到该请求的参数。
数据对应的网页内容为： 测试这个请求。 浏览器可以发起这个请求得到相应，但我们的代码不一定能做到。有一部分原因是网站开发者本身不希望数据被爬取。所以我们需要测试这个接口。这里推荐Postman，先对接口进行测试，查看是否有些Headers或者参数是不需要的，以简化代码量。
Postman的用法下回分解吧。
编写代码。 #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/15 22:11 # @Author : Johnathan Lin import requests import json if __name__ == '__main__': # 请求头，一般写上User-Agent防止爬虫，遇到有验证状态的网站要填写Cookie headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36' } # 请求Url url = 'https://movie."><meta name=author content="JohnathanLin"><link rel=canonical href=https://windypath.com/posts/two_crawl_web_approach/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.eac269fd456bbc5decf5ec3fed956c33a791c95561d9ffc47422c34d673f662d.css integrity="sha256-6sJp/UVrvF3s9ew/7ZVsM6eRyVVh2f/EdCLDTWc/Zi0=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://windypath.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://windypath.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://windypath.com/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://windypath.com/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://windypath.com/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://windypath.com/posts/two_crawl_web_approach/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-MLYM2PFRSJ"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-MLYM2PFRSJ",{anonymize_ip:!1})}</script><meta property="og:title" content="简述爬虫对两种网站的不同爬取方式"><meta property="og:description" content="爬虫的目的是采集网站的数据。而网站渲染数据有两种方式。我个人将其称为前端渲染和后端渲染。
前端渲染 前端渲染指的是网页并不直接展示数据，而是在读取完网页之后，再次向服务器请求数据。在得到数据之后再渲染到网页中。
后端渲染 后端渲染指的是服务器收到请求之后，将数据在后端写入网页，然后将带有数据的网页直接展示在浏览器中。
爬取方式 目前我并没有使用scrapy、webmagic等爬虫框架，仅使用python的requests模块，json模块和BeautifulSoup框架。
前端渲染的爬取方式
步骤：使用requests请求数据，再用json.loads()方法将返回的数据解析，最后操作得到的数据对象即可。
我们这里以豆瓣为例。
分析该网页。 爬取的url：
https://movie.douban.com/explore#!type=movie&amp;tag=%E7%83%AD%E9%97%A8&amp;sort=recommend&amp;page_limit=20&amp;page_start=0
将url放进浏览器，先点开F12，然后访问。这里我使用谷歌浏览器 可以看到，红框内的All、XHR、JS等。这是一个筛选框，用来筛选该网页请求的数据。
All代表所有，XHR代表异步请求，JS代表Js文件，Css……
对于前端渲染，必然有异步的过程，所以选择XHR。
通过观察，发现第三行是“选电影”列表的数据。 这时，我们点击Headers，查看其请求的详细信息。 可以看到，这是一个Get请求，在下面的Qurey String Parameters可以看到该请求的参数。
数据对应的网页内容为： 测试这个请求。 浏览器可以发起这个请求得到相应，但我们的代码不一定能做到。有一部分原因是网站开发者本身不希望数据被爬取。所以我们需要测试这个接口。这里推荐Postman，先对接口进行测试，查看是否有些Headers或者参数是不需要的，以简化代码量。
Postman的用法下回分解吧。
编写代码。 #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/15 22:11 # @Author : Johnathan Lin import requests import json if __name__ == '__main__': # 请求头，一般写上User-Agent防止爬虫，遇到有验证状态的网站要填写Cookie headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36' } # 请求Url url = 'https://movie."><meta property="og:type" content="article"><meta property="og:url" content="https://windypath.com/posts/two_crawl_web_approach/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-11-15T23:05:46+08:00"><meta property="article:modified_time" content="2019-11-15T23:05:46+08:00"><meta property="og:site_name" content="Windypath"><meta name=twitter:card content="summary"><meta name=twitter:title content="简述爬虫对两种网站的不同爬取方式"><meta name=twitter:description content="爬虫的目的是采集网站的数据。而网站渲染数据有两种方式。我个人将其称为前端渲染和后端渲染。
前端渲染 前端渲染指的是网页并不直接展示数据，而是在读取完网页之后，再次向服务器请求数据。在得到数据之后再渲染到网页中。
后端渲染 后端渲染指的是服务器收到请求之后，将数据在后端写入网页，然后将带有数据的网页直接展示在浏览器中。
爬取方式 目前我并没有使用scrapy、webmagic等爬虫框架，仅使用python的requests模块，json模块和BeautifulSoup框架。
前端渲染的爬取方式
步骤：使用requests请求数据，再用json.loads()方法将返回的数据解析，最后操作得到的数据对象即可。
我们这里以豆瓣为例。
分析该网页。 爬取的url：
https://movie.douban.com/explore#!type=movie&amp;tag=%E7%83%AD%E9%97%A8&amp;sort=recommend&amp;page_limit=20&amp;page_start=0
将url放进浏览器，先点开F12，然后访问。这里我使用谷歌浏览器 可以看到，红框内的All、XHR、JS等。这是一个筛选框，用来筛选该网页请求的数据。
All代表所有，XHR代表异步请求，JS代表Js文件，Css……
对于前端渲染，必然有异步的过程，所以选择XHR。
通过观察，发现第三行是“选电影”列表的数据。 这时，我们点击Headers，查看其请求的详细信息。 可以看到，这是一个Get请求，在下面的Qurey String Parameters可以看到该请求的参数。
数据对应的网页内容为： 测试这个请求。 浏览器可以发起这个请求得到相应，但我们的代码不一定能做到。有一部分原因是网站开发者本身不希望数据被爬取。所以我们需要测试这个接口。这里推荐Postman，先对接口进行测试，查看是否有些Headers或者参数是不需要的，以简化代码量。
Postman的用法下回分解吧。
编写代码。 #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/15 22:11 # @Author : Johnathan Lin import requests import json if __name__ == '__main__': # 请求头，一般写上User-Agent防止爬虫，遇到有验证状态的网站要填写Cookie headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36' } # 请求Url url = 'https://movie."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://windypath.com/posts/"},{"@type":"ListItem","position":2,"name":"简述爬虫对两种网站的不同爬取方式","item":"https://windypath.com/posts/two_crawl_web_approach/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"简述爬虫对两种网站的不同爬取方式","name":"简述爬虫对两种网站的不同爬取方式","description":"爬虫的目的是采集网站的数据。而网站渲染数据有两种方式。我个人将其称为前端渲染和后端渲染。\n前端渲染 前端渲染指的是网页并不直接展示数据，而是在读取完网页之后，再次向服务器请求数据。在得到数据之后再渲染到网页中。\n后端渲染 后端渲染指的是服务器收到请求之后，将数据在后端写入网页，然后将带有数据的网页直接展示在浏览器中。\n爬取方式 目前我并没有使用scrapy、webmagic等爬虫框架，仅使用python的requests模块，json模块和BeautifulSoup框架。\n前端渲染的爬取方式\n步骤：使用requests请求数据，再用json.loads()方法将返回的数据解析，最后操作得到的数据对象即可。\n我们这里以豆瓣为例。\n分析该网页。 爬取的url：\nhttps://movie.douban.com/explore#!type=movie\u0026amp;tag=%E7%83%AD%E9%97%A8\u0026amp;sort=recommend\u0026amp;page_limit=20\u0026amp;page_start=0\n将url放进浏览器，先点开F12，然后访问。这里我使用谷歌浏览器 可以看到，红框内的All、XHR、JS等。这是一个筛选框，用来筛选该网页请求的数据。\nAll代表所有，XHR代表异步请求，JS代表Js文件，Css……\n对于前端渲染，必然有异步的过程，所以选择XHR。\n通过观察，发现第三行是“选电影”列表的数据。 这时，我们点击Headers，查看其请求的详细信息。 可以看到，这是一个Get请求，在下面的Qurey String Parameters可以看到该请求的参数。\n数据对应的网页内容为： 测试这个请求。 浏览器可以发起这个请求得到相应，但我们的代码不一定能做到。有一部分原因是网站开发者本身不希望数据被爬取。所以我们需要测试这个接口。这里推荐Postman，先对接口进行测试，查看是否有些Headers或者参数是不需要的，以简化代码量。\nPostman的用法下回分解吧。\n编写代码。 #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/15 22:11 # @Author : Johnathan Lin import requests import json if __name__ == \u0026#39;__main__\u0026#39;: # 请求头，一般写上User-Agent防止爬虫，遇到有验证状态的网站要填写Cookie headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36\u0026#39; } # 请求Url url = \u0026#39;https://movie.","keywords":[],"articleBody":"爬虫的目的是采集网站的数据。而网站渲染数据有两种方式。我个人将其称为前端渲染和后端渲染。\n前端渲染 前端渲染指的是网页并不直接展示数据，而是在读取完网页之后，再次向服务器请求数据。在得到数据之后再渲染到网页中。\n后端渲染 后端渲染指的是服务器收到请求之后，将数据在后端写入网页，然后将带有数据的网页直接展示在浏览器中。\n爬取方式 目前我并没有使用scrapy、webmagic等爬虫框架，仅使用python的requests模块，json模块和BeautifulSoup框架。\n前端渲染的爬取方式\n步骤：使用requests请求数据，再用json.loads()方法将返回的数据解析，最后操作得到的数据对象即可。\n我们这里以豆瓣为例。\n分析该网页。 爬取的url：\nhttps://movie.douban.com/explore#!type=movie\u0026tag=%E7%83%AD%E9%97%A8\u0026sort=recommend\u0026page_limit=20\u0026page_start=0\n将url放进浏览器，先点开F12，然后访问。这里我使用谷歌浏览器 可以看到，红框内的All、XHR、JS等。这是一个筛选框，用来筛选该网页请求的数据。\nAll代表所有，XHR代表异步请求，JS代表Js文件，Css……\n对于前端渲染，必然有异步的过程，所以选择XHR。\n通过观察，发现第三行是“选电影”列表的数据。 这时，我们点击Headers，查看其请求的详细信息。 可以看到，这是一个Get请求，在下面的Qurey String Parameters可以看到该请求的参数。\n数据对应的网页内容为： 测试这个请求。 浏览器可以发起这个请求得到相应，但我们的代码不一定能做到。有一部分原因是网站开发者本身不希望数据被爬取。所以我们需要测试这个接口。这里推荐Postman，先对接口进行测试，查看是否有些Headers或者参数是不需要的，以简化代码量。\nPostman的用法下回分解吧。\n编写代码。 #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/15 22:11 # @Author : Johnathan Lin import requests import json if __name__ == '__main__': # 请求头，一般写上User-Agent防止爬虫，遇到有验证状态的网站要填写Cookie headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36' } # 请求Url url = 'https://movie.douban.com/j/search_subjects?type=movie\u0026tag=%E7%83%AD%E9%97%A8\u0026sort=recommend\u0026page_limit=20\u0026page_start=0' # 发送请求 request = requests.get(url, headers=headers) json_data = str(request.content, encoding='utf-8') # 解析数据 json_data_obj = json.loads(json_data) for data in json_data_obj['subjects']: print(data['title']) 爬取结果： 这里建议养成写上User-Agent的习惯。\n对于Get请求，将数据装在url的“？”后面即可，但对于Post请求，一般建议写在data对象里。\n我们以网站http://ent.zdface.com/为例。\nF12看到一个异步请求： 注意这里同时有Query String Parameters和Form Data。Query String Parameters是装在url后的参数，而Form Data是POST请求传入的参数。\n查看其Preview，发现是某一板块下的数据。 找到网页对应部分内容： 开始爬取，注意这回参数要写进data对象里。\n#!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/15 22:11 # @Author : Johnathan Lin import requests import json if __name__ == '__main__': # 请求头，一般写上User-Agent防止爬虫，遇到有验证状态的网站要填写Cookie headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36' } # 请求参数 data = { 'last': '0', 'amount': '20' } # 请求Url url = 'http://ent.zdface.com/e/ajax/data.php?classid=12' # 发送POST请求 request = requests.post(url, data=data, headers=headers) json_data = str(request.content, encoding='utf-8') # 解析数据 json_data_obj = json.loads(json_data) for data in json_data_obj: print(data['title']) 爬取结果： 后端渲染： 后端渲染网站的爬取过程依旧是发起请求-\u003e解析。\n这里使用BeautifulSoup解析。\n这个框架本身并不难。不需要看什么教程，仔仔细细把官方的中文文档读一遍下来，就知道怎么用了。\n文档地址：https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/\n我选择我自己的网站作为后端渲染的网页爬取样例。\n我 爬 我 自 己\n这是之前用wordpress的时候写的文章，目前可能不适用了。\n目标url：www.windypath.com\nF12看一下。\n一般来说，后端渲染的网站，只要挑第一个看就可以了。而如果你点XHR，将什么都看不到。 这是一个Get请求。点击Response： 可以看到，每一篇文章都是用标签展示的，其中里面在下的标签装着标题内容，标签的href就是文章的url。\n也就是说，我们要获取文章列表url，可以通过标签找到标题的标签。\nBeautifuSoup功能虽强大，但它提供的一些find()，findAll()并不实用，直接实用select()方法，参数为CSS选择器的字符串，像操控CSS选择元素一样找到元素，是比较灵活方便有效的方式。\n注：BeautifulSoup支持许多解析器，用来解析html文档，这里我只用过lxml，记得装bs4的时候也要装lxml。\n于是我们写成以下代码：\n#!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/15 22:11 # @Author : Johnathan Lin import requests import json from bs4 import BeautifulSoup if __name__ == '__main__': # 请求头，一般写上User-Agent防止爬虫，遇到有验证状态的网站要填写Cookie headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36' } # 请求Url url = 'http://www.windypath.com/' # 发送请求 request = requests.get(url, headers=headers) html = str(request.content, encoding='utf-8') # 解析数据 html_content = BeautifulSoup(html, 'lxml') for article in html_content.select('article'): a_tag = article.select('header \u003e h1 \u003e a')[0] print('文章标题：' + a_tag.get_text().strip() + ',文章url：' + a_tag['href']) 输出结果： ","wordCount":"301","inLanguage":"zh","datePublished":"2019-11-15T23:05:46+08:00","dateModified":"2019-11-15T23:05:46+08:00","author":{"@type":"Person","name":"JohnathanLin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://windypath.com/posts/two_crawl_web_approach/"},"publisher":{"@type":"Organization","name":"风萧古道 - 勤学苦练，年复一年","logo":{"@type":"ImageObject","url":"https://windypath.com/%3Clink%20/%20abs%20url%3E"}}}</script><script src=https://cdn.bootcdn.net/ajax/libs/jquery/3.6.4/jquery.min.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-MLYM2PFRSJ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-MLYM2PFRSJ")</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://windypath.com/ accesskey=h title="风萧古道 (Alt + H)">风萧古道</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://windypath.com/en/ title="Switch to English Page" aria-label="Switch to English Page">En</a></li></ul></div></div><ul id=menu><li><a href=https://windypath.com/archives/ title=归档><span>归档</span></a></li><li><a href=https://windypath.com/categories/ title=分类><span>分类</span></a></li><li><a href=https://windypath.com/about/ title=关于><span>关于</span></a></li><li><a href=https://windypath.com/mybooks/ title=藏书><span>藏书</span></a></li><li><a href=https://windypath.com/myfriends/ title=朋友><span>朋友</span></a></li><li><a href=https://windypath.com/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>简述爬虫对两种网站的不同爬取方式</h1><div class=post-meta>&lt;span title='2019-11-15 23:05:46 +0800 +0800'>十一月 15, 2019&lt;/span>&amp;nbsp;·&amp;nbsp;JohnathanLin</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e5%89%8d%e7%ab%af%e6%b8%b2%e6%9f%93 aria-label=前端渲染>前端渲染</a></li><li><a href=#%e5%90%8e%e7%ab%af%e6%b8%b2%e6%9f%93 aria-label=后端渲染>后端渲染</a></li><li><a href=#%e7%88%ac%e5%8f%96%e6%96%b9%e5%bc%8f aria-label=爬取方式>爬取方式</a><ul><li><a href=#%e5%88%86%e6%9e%90%e8%af%a5%e7%bd%91%e9%a1%b5 aria-label=分析该网页。>分析该网页。</a></li><li><a href=#%e6%b5%8b%e8%af%95%e8%bf%99%e4%b8%aa%e8%af%b7%e6%b1%82 aria-label=测试这个请求。>测试这个请求。</a></li><li><a href=#%e7%bc%96%e5%86%99%e4%bb%a3%e7%a0%81 aria-label=编写代码。>编写代码。</a></li></ul></li><li><a href=#%e5%90%8e%e7%ab%af%e6%b8%b2%e6%9f%93-1 aria-label=后端渲染：>后端渲染：</a></li></ul></div></details></div><div class=post-content><p>爬虫的目的是采集网站的数据。而网站渲染数据有两种方式。我个人将其称为前端渲染和后端渲染。</p><h2 id=前端渲染>前端渲染<a hidden class=anchor aria-hidden=true href=#前端渲染>#</a></h2><p>前端渲染指的是网页并不直接展示数据，而是在读取完网页之后，再次向服务器请求数据。在得到数据之后再渲染到网页中。</p><h2 id=后端渲染>后端渲染<a hidden class=anchor aria-hidden=true href=#后端渲染>#</a></h2><p>后端渲染指的是服务器收到请求之后，将数据在后端写入网页，然后将带有数据的网页直接展示在浏览器中。</p><h2 id=爬取方式>爬取方式<a hidden class=anchor aria-hidden=true href=#爬取方式>#</a></h2><p>目前我并没有使用scrapy、webmagic等爬虫框架，仅使用python的requests模块，json模块和BeautifulSoup框架。</p><p>前端渲染的爬取方式</p><p>步骤：使用requests请求数据，再用json.loads()方法将返回的数据解析，最后操作得到的数据对象即可。</p><p>我们这里以豆瓣为例。</p><h3 id=分析该网页>分析该网页。<a hidden class=anchor aria-hidden=true href=#分析该网页>#</a></h3><p>爬取的url：</p><p><a href="https://movie.douban.com/explore#!type=movie&amp;tag=%E7%83%AD%E9%97%A8&amp;sort=recommend&amp;page_limit=20&amp;page_start=0">https://movie.douban.com/explore#!type=movie&amp;tag=%E7%83%AD%E9%97%A8&amp;sort=recommend&amp;page_limit=20&amp;page_start=0</a></p><p>将url放进浏览器，先点开F12，然后访问。这里我使用谷歌浏览器
<img loading=lazy src=/images/crawl1.png alt></p><p>可以看到，红框内的All、XHR、JS等。这是一个筛选框，用来筛选该网页请求的数据。</p><p>All代表所有，XHR代表异步请求，JS代表Js文件，Css……</p><p>对于前端渲染，必然有异步的过程，所以选择XHR。</p><p>通过观察，发现第三行是“选电影”列表的数据。
<img loading=lazy src=/images/crawl2.png alt></p><p>这时，我们点击Headers，查看其请求的详细信息。
<img loading=lazy src=/images/crawl3.png alt>
可以看到，这是一个Get请求，在下面的Qurey String Parameters可以看到该请求的参数。</p><p>数据对应的网页内容为：
<img loading=lazy src=/images/crawl4.png alt></p><h3 id=测试这个请求>测试这个请求。<a hidden class=anchor aria-hidden=true href=#测试这个请求>#</a></h3><p>浏览器可以发起这个请求得到相应，但我们的代码不一定能做到。有一部分原因是网站开发者本身不希望数据被爬取。所以我们需要测试这个接口。这里推荐Postman，先对接口进行测试，查看是否有些Headers或者参数是不需要的，以简化代码量。</p><p>Postman的用法下回分解吧。</p><h3 id=编写代码>编写代码。<a hidden class=anchor aria-hidden=true href=#编写代码>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- coding: utf-8 -*-</span>
</span></span><span style=display:flex><span><span style=color:#75715e># @Time    : 2019/11/15 22:11</span>
</span></span><span style=display:flex><span><span style=color:#75715e># @Author  : Johnathan Lin</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># 请求头，一般写上User-Agent防止爬虫，遇到有验证状态的网站要填写Cookie</span>
</span></span><span style=display:flex><span>    headers <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;User-Agent&#39;</span>: <span style=color:#e6db74>&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36&#39;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#75715e># 请求Url</span>
</span></span><span style=display:flex><span>    url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;https://movie.douban.com/j/search_subjects?type=movie&amp;tag=</span><span style=color:#e6db74>%E</span><span style=color:#e6db74>7</span><span style=color:#e6db74>%83%</span><span style=color:#e6db74>AD</span><span style=color:#e6db74>%E</span><span style=color:#e6db74>9</span><span style=color:#e6db74>%97%</span><span style=color:#e6db74>A8&amp;sort=recommend&amp;page_limit=20&amp;page_start=0&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 发送请求</span>
</span></span><span style=display:flex><span>    request <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(url, headers<span style=color:#f92672>=</span>headers)
</span></span><span style=display:flex><span>    json_data <span style=color:#f92672>=</span> str(request<span style=color:#f92672>.</span>content, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;utf-8&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 解析数据</span>
</span></span><span style=display:flex><span>    json_data_obj <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(json_data)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> data <span style=color:#f92672>in</span> json_data_obj[<span style=color:#e6db74>&#39;subjects&#39;</span>]:
</span></span><span style=display:flex><span>        print(data[<span style=color:#e6db74>&#39;title&#39;</span>])
</span></span></code></pre></div><p>爬取结果：
<img loading=lazy src=/images/crawl5.png alt>
这里建议养成写上User-Agent的习惯。</p><p>对于Get请求，将数据装在url的“？”后面即可，但对于Post请求，一般建议写在data对象里。</p><p>我们以网站http://ent.zdface.com/为例。</p><p>F12看到一个异步请求：
<img loading=lazy src=/images/crawl6.png alt></p><p>注意这里同时有Query String Parameters和Form Data。Query String Parameters是装在url后的参数，而Form Data是POST请求传入的参数。</p><p>查看其Preview，发现是某一板块下的数据。
<img loading=lazy src=/images/crawl7.png alt></p><p>找到网页对应部分内容：
<img loading=lazy src=/images/crawl8.png alt>
开始爬取，注意这回参数要写进data对象里。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- coding: utf-8 -*-</span>
</span></span><span style=display:flex><span><span style=color:#75715e># @Time    : 2019/11/15 22:11</span>
</span></span><span style=display:flex><span><span style=color:#75715e># @Author  : Johnathan Lin</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># 请求头，一般写上User-Agent防止爬虫，遇到有验证状态的网站要填写Cookie</span>
</span></span><span style=display:flex><span>    headers <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;User-Agent&#39;</span>: <span style=color:#e6db74>&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36&#39;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#75715e># 请求参数</span>
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;last&#39;</span>: <span style=color:#e6db74>&#39;0&#39;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;amount&#39;</span>: <span style=color:#e6db74>&#39;20&#39;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#75715e># 请求Url</span>
</span></span><span style=display:flex><span>    url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;http://ent.zdface.com/e/ajax/data.php?classid=12&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 发送POST请求</span>
</span></span><span style=display:flex><span>    request <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>post(url, data<span style=color:#f92672>=</span>data, headers<span style=color:#f92672>=</span>headers)
</span></span><span style=display:flex><span>    json_data <span style=color:#f92672>=</span> str(request<span style=color:#f92672>.</span>content, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;utf-8&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 解析数据</span>
</span></span><span style=display:flex><span>    json_data_obj <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(json_data)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> data <span style=color:#f92672>in</span> json_data_obj:
</span></span><span style=display:flex><span>        print(data[<span style=color:#e6db74>&#39;title&#39;</span>])
</span></span></code></pre></div><p>爬取结果：
<img loading=lazy src=/images/crawl9.png alt></p><h2 id=后端渲染-1>后端渲染：<a hidden class=anchor aria-hidden=true href=#后端渲染-1>#</a></h2><p>后端渲染网站的爬取过程依旧是发起请求->解析。</p><p>这里使用BeautifulSoup解析。</p><p>这个框架本身并不难。不需要看什么教程，仔仔细细把官方的中文文档读一遍下来，就知道怎么用了。</p><p>文档地址：https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/</p><p>我选择我自己的网站作为后端渲染的网页爬取样例。</p><p>我 爬 我 自 己</p><blockquote><p>这是之前用wordpress的时候写的文章，目前可能不适用了。</p></blockquote><p>目标url：www.windypath.com</p><p>F12看一下。</p><p>一般来说，后端渲染的网站，只要挑第一个看就可以了。而如果你点XHR，将什么都看不到。
<img loading=lazy src=/images/crawl10.png alt></p><p>这是一个Get请求。点击Response：
<img loading=lazy src=/images/crawl11.png alt></p><p>可以看到，每一篇文章都是用<article>标签展示的，其中里面在<h1>下的<a>标签装着标题内容，<a>标签的href就是文章的url。</p><p>也就是说，我们要获取文章列表url，可以通过<article>标签找到标题的<a>标签。</p><p>BeautifuSoup功能虽强大，但它提供的一些find()，findAll()并不实用，直接实用select()方法，参数为CSS选择器的字符串，像操控CSS选择元素一样找到元素，是比较灵活方便有效的方式。</p><p>注：BeautifulSoup支持许多解析器，用来解析html文档，这里我只用过lxml，记得装bs4的时候也要装lxml。</p><p>于是我们写成以下代码：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- coding: utf-8 -*-</span>
</span></span><span style=display:flex><span><span style=color:#75715e># @Time    : 2019/11/15 22:11</span>
</span></span><span style=display:flex><span><span style=color:#75715e># @Author  : Johnathan Lin</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> bs4 <span style=color:#f92672>import</span> BeautifulSoup
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># 请求头，一般写上User-Agent防止爬虫，遇到有验证状态的网站要填写Cookie</span>
</span></span><span style=display:flex><span>    headers <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;User-Agent&#39;</span>: <span style=color:#e6db74>&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36&#39;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#75715e># 请求Url</span>
</span></span><span style=display:flex><span>    url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;http://www.windypath.com/&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 发送请求</span>
</span></span><span style=display:flex><span>    request <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(url, headers<span style=color:#f92672>=</span>headers)
</span></span><span style=display:flex><span>    html <span style=color:#f92672>=</span> str(request<span style=color:#f92672>.</span>content, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;utf-8&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 解析数据</span>
</span></span><span style=display:flex><span>    html_content <span style=color:#f92672>=</span> BeautifulSoup(html, <span style=color:#e6db74>&#39;lxml&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> article <span style=color:#f92672>in</span> html_content<span style=color:#f92672>.</span>select(<span style=color:#e6db74>&#39;article&#39;</span>):
</span></span><span style=display:flex><span>        a_tag <span style=color:#f92672>=</span> article<span style=color:#f92672>.</span>select(<span style=color:#e6db74>&#39;header &gt; h1 &gt; a&#39;</span>)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#39;文章标题：&#39;</span> <span style=color:#f92672>+</span> a_tag<span style=color:#f92672>.</span>get_text()<span style=color:#f92672>.</span>strip() <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;,文章url：&#39;</span> <span style=color:#f92672>+</span> a_tag[<span style=color:#e6db74>&#39;href&#39;</span>])
</span></span></code></pre></div><p>输出结果：
<img loading=lazy src=/images/crawl12.png alt></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://windypath.com/posts/regular_expression/><span class=title>« 上一页</span><br><span>java正则表达式 - 双反斜杠（\）和Pattern的matches()与find()</span>
</a><a class=next href=https://windypath.com/posts/why_vue_router_back/><span class=title>下一页 »</span><br><span>Vue的路由配置及手动改地址栏为啥又跳转回来？？</span></a></nav></footer></article></main><footer class=footer><span>Windypath 风萧古道 <strong>For Chinese Software</strong>. <a href=https://beian.miit.gov.cn/>闽ICP备15016446号-3</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
.</span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>